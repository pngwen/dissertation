\documentclass{article}
\usepackage[splitindex]{imakeidx}
\title{Tensor Influence Modelling}

\usepackage{noweb}
\pagestyle{noweb}
\noweboptions{}
\begin{document}
\maketitle
\tableofcontents

\section{Introduction}
This is a collection of classes and utilities which are used to create
tensor representations of corpora of documents.  These tensors are
then analyzed in order to create an influence model among the several
documents in the corpus. 

TODO: More writeup of what I am doing here


\section{Vocabulary}
The first class to be created is one to represent the vocabulary of
the corpus.  For our purposes, the vocabulary of the corpus is
a bijective function from terms to index numbers.  The index numbers
are zero based.  

The overall layout of the vocabulary file is as follows:
<<vocabulary.h>>=
#include <vector>
#include <map>
#include <string>
#include <functional>
#include <algorithm>
#include <iostream>

#ifndef VOCABULARY_H
#define VOCABULARY_H
class Vocabulary
{
public:
    <<Vocabulary Constructors>>
    <<Vocabulary Methods>>
    <<Vocabulary Operators>>

private:
    <<Vocabulary Data>>
};


<<Vocabulary IO Functions>>
#endif
@

The implementation file layout is simple, at least in the beginning.
<<vocabulary.cpp>>=
#include "vocabulary.h"
@

\subsection{Vocabulary Storage}
Vocabularies are maps of words, represented here by strings, onto
index numbers. Because we we want to establish a bidirectional
mapping, we need to store two maps in order to have speedy lookups in
both directions.

<<Vocabulary Data>>=
std::map<std::string, int> toNum;
std::map<int, std::string> toTerm;
@

Finally, we need to store the next available number for automatically
assigning numbered indexes.

<<Vocabulary Data>>=
int nextNum;    //the next available index number
@

\subsection{Vocabulary Construction}
As is common practice, we first provide a no argument constructor.

<<Vocabulary Constructors>>=
Vocabulary();   //no argument constructor
@

The implementation of this constructor simply initializes the nextNum
index.
<<vocabulary.cpp>>=
Vocabulary::Vocabulary() 
{
    nextNum = 0;   //begin at zero
}
@

We also should create a copy constructor.  This is a little more
complex than most because we also need to establish the pointer links.
Because this will also be needed for an assignment operator, we will
implement this using the assignment operator.

<<Vocabulary Constructors>>=
Vocabulary(const Vocabulary &rhs);
@

<<Vocabulary Operators>>=
Vocabulary& operator=(const Vocabulary &rhs);
@

<<vocabulary.cpp>>=
Vocabulary::Vocabulary(const Vocabulary &rhs) : Vocabulary()
{
    *this = rhs;
}
@

For the assignment operator, and therefore also the copy constructor,
we carry out the following:
<<vocabulary.cpp>>=
Vocabulary& Vocabulary::operator=(const Vocabulary &rhs) 
{
    <<Clear the Maps>>
    <<Copy the Terms>>

    return *this;
}
@

The first thing to do in the assignment operator is to clear all the
maps and reset the nextNum field.
<<Clear the Maps>>=
    toNum.clear();
    toTerm.clear();
    nextNum = 0;
@

Next we simply add all the terms from the rhs.
<<Copy the Terms>>=
for(auto itr=rhs.toNum.begin(); itr != rhs.toNum.end(); itr++) {
    add(itr->first, itr->second);
}
@


\subsection{Adding Terms}
The primary operation of the vocabulary is to acception new terms.
This can be done with either the autoassignment of a number, or
a number can be specified (for example, if an existing vocabulary were
being loaded.)
<<Vocabulary Methods>>=
void add(const std::string &term);
void add(const std::string &term, int num);
@

Of the two, the most generic is the method which adds with respect to
a given number.  To that end, the first can be written in terms of the
second one.
<<vocabulary.cpp>>=
void Vocabulary::add(const std::string &term)
{
    //add using the nextNum as the number
    add(term, nextNum);
}
@

Now for the task of adding the number by value.  
<<vocabulary.cpp>>=
void Vocabulary::add(const std::string &term, int num)
{
    <<Verify Uniqueness>>
    <<Populate the Maps>>
    <<Advance nextNum>>
}
@

Every term and number within the vocabulary must be unique, and so we
must check for the existence of either.  If either are found to be
already present, we return without inserting anything.  
<<Verify Uniqueness>>=
if(toNum.find(term) != toNum.end() or toTerm.find(num) != toTerm.end()) 
{
   return;
}
@


Populating the maps is also fairly simple.  
<<Populate the Maps>>=
toNum[term] = num;
toTerm[num] = term;
@

Finally, we need to advance the number.  The logic is that we want
nxtNum to be the largest number in play, so if we've just inserted
a larger one, we increment it to get the next largest number.
<<Advance nextNum>>=
if(num >= nextNum) {
    nextNum = num+1;
}
@

We may also be interested in the number of items in the vocabulary.
<<Vocabulary Methods>>=
int size() const;
@

<<vocabulary.cpp>>=
int Vocabulary::size() const
{
    return toTerm.size();
}
@

\subsection{Iterators}
Because the {\tt Vocabulary} object is really just a bidirectional
wrapper for {\tt std::map}, we can allow iteration using the {\tt
std::map::iterator} objects.

<<Vocabulary Methods>>=
std::map<std::string, int>::iterator begin();
std::map<std::string, int>::iterator end();
@

Both of these will just use the toNum map.  The real aim of this
activity is to allow the user to step through all terms in the
vocabulary.

<<vocabulary.cpp>>=
std::map<std::string, int>::iterator Vocabulary::begin()
{
    return toNum.begin();
}

std::map<std::string, int>::iterator Vocabulary::end()
{
    return toNum.end();
}
@

Of course, we are playing with fire here.  We could alter the string
at the end of the pointer, but we shouldn't do that, so we won't.

\subsection{Vocabulary Functors}
The final addition to the vocabulary class is the functor.  This is
the set of operators which map terms to numbers and numbers to terms.

<<Vocabulary Operators>>=
int operator()(const std::string &term);
std::string operator()(int num);
@

These are, of course trivial given the maps.

<<vocabulary.cpp>>=
int Vocabulary::operator()(const std::string &term) 
{
    return toNum[term];
}
@


<<vocabulary.cpp>>=
std::string Vocabulary::operator()(int num) 
{
    return toTerm[num];
}
@


\subsection{Vocabulary IO Functions}
In addition to the vocabulary object, we need some way to read and
write vocabulary objects from iostream.  Our convention for
translating them is to state the term followed by the term's number.
This is, of course, quite easy to do.  The signature of the Input and
Output functions are:

<<Vocabulary IO Functions>>=
Vocabulary readVocabulary(std::istream &is);
void writeVocabulary(std::ostream &os, Vocabulary &v);
@

Reading is just a matter of repeatedly calling "add" until the stream
ends.

<<vocabulary.cpp>>=
Vocabulary readVocabulary(std::istream &is) 
{
    Vocabulary v;
    std::string term;
    int num;

    //read until the end of the stream (or first error)
    while(is >> term>> num) {
        v.add(term, num);
    }

    return v;
}
@

Writing is just a simple act of iteration. We put each pair on a line
by itself with a tab delimiter in between.

<<vocabulary.cpp>>=
void writeVocabulary(std::ostream &os, Vocabulary &v)
{
    for(auto itr = v.begin(); itr != v.end(); itr++) {
        os << itr->first << '\t' << itr->second << std::endl;
    }
}
@

\subsection{vocab - Filter Vocabulary Extractor}
In this section, we will create a filter program which extracts the
vocabulary from the stdin stream.  It uses {\tt cin} to extract
strings and treats them as terms.  As such, it breaks terms on
whitespace characters.  Once the stream ends, it prints out the
vocabulary.  The layout of the program is as follows.

<<vocab.cpp>>=
/* A filter tool for extracting a vocabulary from stdin.
 */
#include <iostream>
#include <string>
#include "vocabulary.h"

using namespace std;

int main()
{
    Vocabulary v;
    string term;

    <<Extract Vocabulary>>

    writeVocabulary(cout, v);
}
@

Extracting the vocabulary is fairly straightforward.  We just use add
all the terms from the stream.

<<Extract Vocabulary>>=
while(cin >> term) {
    v.add(term);
}
@

\section{Corpus Processing}
The corpus which we are modelling consists of a directory structure,
where each document has its own directory. The corpus is built from
a series of PDF documents which are converted into preprocessed text
documents.  The preprocessed text documents are then converted into
tesnors, which are in turn decomposed and modelled. In addition to the
files, the corpus also has a vocabulary which is extracted from the
text files.  The final directory structure of a corpus is as follows:
\begin{itemize}
    \item {\tt corpusName/}
    \begin{itemize}
        \item {\tt vocabulary} - The Corpus Vocabulary
        \item {\tt docName/}  - Document Directories (one per docName)
        \begin{itemize}
            \item {\tt docName.pdf} - The source document
            \item {\tt docName.txt} - The preprocessed text
            \item {\tt docName.tns} - Document Tensor
            \item mode\#.mat - Decomposed Modes of the Tensor
            \item lambda.mat - The Lambdas for the Decomposition
        \end{itemize}
    \end{itemize}
\end{itemize}

\subsection{build-corpus}
In this section, we construct a script which will build the corpus
directory structure.  This script performs the following operations:

<<build-corpus>>=
#!/bin/sh
#Build a corpus in the current directory.  The PDF files are specified
#as command line arguments

#loop over all the pdf documents
for file in "$@"
do
    <<Create Document Directory>>
    <<Construct Preprocessed Text File>>
done

<<Create Corpus Vocabulary>>
@

It should be noted before each chunk is defined that this script
assumes that all of the utilities outlined in this document are within
the execution path of the script.

The first step is to process all of the files into document
directories.  This is done simply by looping over all the pdf files
and extracting the filename, without extension, from them.
<<Create Document Directory>>=
#extract filename
fname="$(basename $file | cut -d. -f1)"

#create the directory
mkdir "$fname"

#copy the pdf file
cp "$file" "$fname"/.
@

Preprocessing is accomplished via pdftotext and tr.  We remove all
characters except alpha and space characters.  We then convert
everything to lowercase.

<<Construct Preprocessed Text File>>=
pdftotext "$fname/$fname.pdf" - | \
   tr -cd '[:alpha:][:space:]' | \
   tr [:upper:] [:lower:] > "$fname/$fname.txt"
@

Finally we extract the vocabulary by concatenating all the text files
through the vocab filter.

<<Create Corpus Vocabulary>>=
cat `find . -name '*.txt'` | vocab > vocabulary
@

\subsection{doctns - Document Tensors}
The {\tt doctns} utility converts documents into tensors.  The tensor
representation of documents in this model are based on $n$-grams,
where $n$ is the number of modes in the tensor.  The tensor is
a $\mathtt{TERM}_1 \times \mathtt{TERM}_2 \times \ldots
\times \mathtt{TERM}_n$ tensor.  The tensor can handle $n$-grams of
any length, but the model depends on a constant size for the tensor
throughout the corpose.  Hence we can do 3-gram, 4-gram, 5-gram, etc.
but it must be the same for each document in the corpus.  

The tensors count the frequency of each $n$-gram as follows.  For ease
of explanation, we will focus on a 3-gram model.  Tensor element
$T_{ijk}$ would be the frequency of the 3-gram formed by the sequence
of words {\tt vocabulary($i) vocabualry($j$) vocabulary($j$)}. Note
that orderingof the words within the $n$-gram is a distinguishing
feature of the 3-grams.  

The {\tt doctns} utility creates this tensor based on the desired
$n$-gram length and the given vocabulary.  The basic outline of the
program is as follows.

<<doctns.cpp>>=
/* A utility to convert a document into a tensor.
   The command line arguments are: n vocabulary document.
   The output is printed to standard out. */
#include <iostream>
#include <fstream>
#include <cstdlib>
#include <vector>
#include <string>
#include "tensor.h"
#include "vocabulary.h"

using namespace std;

int main(int argc, char** argv)
{
    <<doctns Variables>>

    <<Process doctns Arguments>>
    <<Count n-grams>>
    <<Print the Document Tensor>>
}
@
\end{document}


\subsubsection{Processing Arguments}
The first step is to process the command line arguments.
<<Process doctns Arguments>>=
<<Check doctns Usage>>
<<Load doctns Vocabulary>>
<<Instantiate doctns Tensor>>
<<Open Document File>>
@

doctns requires 4 arguments, so we just need to verify that we have
them all!
<<Check doctns Usage>>=
//handle errors
if(argc != 4) {
    cerr << "Usage: " << argv[0] << " n vocabulary document" << endl;
    exit(1);
}
@


To load the vocabulary, we open the file and then read the vocabulary
in.
<<doctns Variables>>=
Vocabulary v;     //the vocabulary
ifstream file;    //file for input
@

<<Load doctns Vocabulary>>=
//open vocabulary file and check for errors
file.open(argv[2]);
if(!file) {
    cerr << "Could not open vocabulary file: " << argv[2] << endl;
    exit(3);
}

//read the vocabulary file
v = readVocabulary(file);

file.close();
@

To instantiate the tensor, we need to build a tensor index where we
repeat the size of the vocabulary $n$ tims.

<<doctns Variables>>=
int n;                //the number of modes
Tensor<>::Index dim;  //the tensor dimensions
Tensor<> t{};           //the document tensor
@

<<Instantiate doctns Tensor>>=
//get n-gram size
n = atoi(argv[1]);

//build the tensor dimensions
for(int i=0; i<n; i++) {
    dim.append(v.size());
}

//create the tensor
t = Tensor<>(dim);
@


Finally, we open the document file, checking for errors.
<<Open Document File>>=
//open document and handle errors
file.open(argv[3]);
if(!file) {
    cerr << "Could not open document file: " << argv[3] << endl;
    exit(2);
}
@


\subsubsection{$n$-gram counting}
Now we come to the crux of the matter, the counting of $n$-grams.  We
will be using as sliding window for the adjacent terms, and as such it
is convenient to load the file into a vector so we can then count the
$n$-grams.  This process then becomes:

<<Count n-grams>>=
<<Load Document Vector>>
<<n-gram sliding window>>
@

Loading the document vector is just simply consuming everything in the
file.

<<doctns Variables>>=
string word;
vector<string> doc;
@

<<Load Document Vector>>=
while(file >> word) {
    doc.push_back(word);
}

//we are done with the file!
file.close();
@

The sliding window will start at position 0, and it will continue
until it gets to {\tt doc.size()-n}.  Each step along the way, we will
build an index into the tensor and use this to update the frequency of
each $n$-gram.

<<doctns Variables>>=
Tensor<>::Index idx;
@

<<n-gram sliding window>>=
idx = Tensor<>::Index(n);  //create an n-element index
for(int i=0; i<doc.size()-n; i++) {
    //build the n-gram index from vocabulary and document
    for(int j=0; j<n; j++) { 
        idx[j] = v(doc[i+j]);
    }

    //add 1 to the frequency
    t[idx] += 1;
}
@

\subsubsection{Output}
Now we are ready for the output of doctns.  This is a simple operation
using the sparse iterator provided in the tensor class.


<<Print the Document Tensor>>=
for(auto itr = t.sparseBegin(); itr != t.sparseEnd(); itr++) {
    if(*itr < 2.0) continue;
    //print the index
    idx=itr.index();
    <<Print Tensor Index>>

    //print the element
    cout << *itr << endl;
}
@


<<Print Tensor Index>>=
for(int i=0; i<idx.size(); i++) {
    cout << idx[i] << '\t';
}
@


There is one last little peculiarity to deal with.  In order to
preserve the correct dimensions for the tensor, if the corners are not
present in the sparse data, we must insert them.

<<Print the Document Tensor>>=
//handle the leading corner
idx = t.begin().index();
if(t[idx] <= 1.0) {
    <<Print Tensor Index>>
    cout << 0.0 << endl;
}

//handle the trailing corner
idx = (t.end()-1).index();
if(t[idx] <= 1.0) {
    <<Print Tensor Index>>
    cout << 0.0 << endl;
}
@
