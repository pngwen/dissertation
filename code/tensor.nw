\documentclass{article}
\usepackage[splitindex]{imakeidx}
\makeindex[name=functions,title=Functions and Classes]
\title{Tensor and Tensor Operations}

\usepackage{noweb}
\pagestyle{noweb}
\noweboptions{}
\begin{document}
\maketitle
\tableofcontents

\section{Introduction}
This is a template of a tensor class.  A tensor is a multi-dimensional
array which is a generalization of matrices, vectors, and scalars.
A scalar can be represented as an order 0 tensor, a vector is an order 1 
tensor, and a matrix is an order 2 tensor.  We can extend the notion of
tensor above two dimensions by imagining prisms, and hyperprisms of data.  

The elements of a tensor are scalar values, and it is useful to be able to 
use a wide range of numeric types to define these arrays.  This
is why the present class is a template.  A tensor could be comprised of int, 
double, or of some sort of arbitrary precision arithmetic.  Regardless of
the object stored in the tensor, the overall handling of the tensors remains
the same, though some functions will need to be passed into the template for
the operations to work.  By default, the tensor will operate on doubles, and
so all the template parameters will be geared toward that.


The template specification for this tensor is as follows:

<<Includes>>=
#include <cmath>
@

<<Tensor Template>>=
template<typename E=double, E(*SQRT)(E)=std::sqrt>
@

Where {\tt E} is the element type and {\tt SQRT} is a function pointer 
type for a  square root function for the elements.  The overall layout of 
the template file (named ``{\tt tensor.h}'') is as follows:

In order to conserve memory, this tensor template uses sparse storage.
This has an impact on the selection of {\tt E} as the algorithms used
throughout this template assume that the default value of type {\tt E}
functions mathematically as a zero.  That is, the usual rules around
zero should befollowed.  The key principles which must be adhered
to in the selection of {\tt E} are:
\begin{itemize}
    \item {\tt x * E() == 0} for all values of {\tt x}
    \item {\tt x + E() == x} for all values of {\tt x}
    \item {\tt x - E() == x} for all values of {\tt x}
    \item {\tt E() - x == -x} for all values of {\tt x}
\end{itemize}
All of the algorithms which follow will make this basic assumption.
Violating them could have some interesting results!


<<tensor.h>>=
#ifndef TENSOR_H
#define TENSOR_H
<<Includes>>

<<Tensor Template>> class Tensor
{
public:
    //inner class prototypes
    <<Tensor Inner Class Prototypes>>
    
    //constructors
    <<Tensor Constructors>>
    
    //scalar operations
    <<Tensor Scalar Operations>>
    
    //tensor operations
    <<Tensor Operations>>
    
    //inner classes
    <<Tensor Inner Classes>>
    
private:
    <<Tensor Member Variables>>
};

#endif
@

\section{Storage}
The first question to be addressed in construction this tensor is, how does
tensor storage work?  One approach would be to simply use vectors of vectors
nested enough levels deep to hold all of the elements.  This would be the
simplest approach, however it is wasteful unless tensors are completely 
populated.  That is, a vector representation is only appropriate for dense
tensors.  Sparse tensors, on the other hand, would leave a lot of empty
space in that configuration.  As this tensor class is being created for a 
class of problems where interesting tensors are both sparse and large, 
another method needs to be used.  

Our method of storage will be to use a map to store the elements of our 
tensor. In order to avoid nesting, we will map an index onto each element,
thus our storage becomes:

<<Includes>>=
#include <map>
@

<<Tensor Member Variables>>=
std::map<Index, E> data;
@

\subsection{Tensor Index}
Of course,this creates the issue of what a tensor index looks like!  
A tensor may have any number of indices, so it should be something like
a vector, but in order to make effecient usage of the map, it should
have basic comparison operations.  Because no pre-existing container
will suffice, we need to make one.  This will be a pretty simple
inner class.

<<Tensor Inner Class Prototypes>>=
class Index;
@

<<Tensor Inner Classes>>=
class Index
{
public:
  //constructors
  <<Index Constructors>>
  
  //vector operations
  <<Index Vector Operations>>
  
  //comparison operations
  <<Index Comparison Operations>>
  
private:
  <<Index Member Variables>>
};
@

Because {\tt Index} is essentially a wrapper for a vector of 
integers, it follows that the inner storage container should
be a vector which is named {\tt v}.

<<Includes>>=
#include <vector>
@

<<Index Member Variables>>=
std::vector<int> v;
@

Construction of an index can be accomplished in multiple ways.  For example,
we could simply pass a vector into a constructor:
<<Index Constructors>>=
Index(std::vector<int> index) : v(index.begin(), index.end()) { }
@
We could also use the an {\tt initializer\_list}, introduced in C++11, 
to initialize the Index:
<<Includes>>=
#include <initializer_list>
@

\index[functions]{\tt Tensor::Index::Index(il)}
<<Index Constructors>>=
Index(std::initializer_list<int> il) : v(il.begin(), il.end()) { }
@

For some tensor operations, we need to be able to create dimensions
and indexes from several index lists.  This is a relatively simple
task, an will be made even better by using an initializer list of
indices.
\index[functions]{\tt Tensor::Index::Index(std::initializer\_list<Index>)}

<<Index Constructors>>=
Index(std::initializer_list<Index> il) 
{
    //append the lists
    for(auto itr = il.begin(); itr != il.end(); itr++) {
        append(*itr);
    }
}
@

It may also be convenient to initialize an index with a given number
of elements.  The user can specify the fill value for the vector, but we will
use -1 (an invalid index) as the default.

\index[functions]{\tt Tensor::Index::Index(n, val=-1)}
<<Index Constructors>>=
Index(int n, int val=-1) : v(n, val) { }
@

Finally, we provide a no-argument constructor that allows the creation of a
zero-dimensional index.  It's not useful in and of itself, but rather
creates an object we are expecting to append to or overwrite.
\index[functions]{\tt Tensor::Index::Index()}
<<Index Constructors>>=
Index() { /* do nothing */ }
@

Because the index is a wrapper for {\tt std::vector}, we should expose some
of the vector operations.  Really we just want to allow indexing and size
to be used.

\index[functions]{\tt Tensor::Index::operator[](i)}
\index[functions]{\tt Tensor::Index::size()}
<<Index Vector Operations>>=
virtual int& operator[](int i) { return v[i]; }
virtual std::vector<int>::size_type size() const { return v.size(); }
@

Another useful vector operation for an index is to allow appending to
the index.  One scenario would be to add a new individual element to
the index.

\index[functions]{\tt Tensor::Index::append(i)}
<<Index Vector Operations>>=
virtual void append(int i) 
{
    v.push_back(i);
}
@

The other key type of appending is to append an entire index onto the
end of an existing one.  This type of operation is common in some
tensor operations

\index[functions]{\tt Tensor::Index::append(rhs)}
<<Index Vector Operations>>=
virtual void append(const Index& rhs) 
{
    v.reserve(v.size() + rhs.size());
    v.insert(v.end(), rhs.v.begin(), rhs.v.end());
}
@

And now that brings us to comparison opeations!  Of course, in order for
map to work, we have to have the {\tt <} operator so that the items can
be sorted.  We also will likely be interested in basic equality and
greater than operations.  To implement the operators, we will first
create a compare function, which will compare the present object to the
right hand sign parameter.  This function will return -1 for less than,
0 for equal, and 1 for greater than.

The comparison of the n-tuples comprising the indices works as follows.
First, we will compare by size, and then by elements.  The basic 
assumption is that the indices are equal.

\index[functions]{\tt Tensor::Index::compare(rhs)}
<<Index Comparison Operations>>=
int compare(const Index &rhs) const
{
   <<Index Size Comparison>>
   <<Index Element Comparison>>
   
   return 0;
}
@

Comparison by size is very simple.  If there are uneven numbers of elements
then the shorter index is said to be the lesser one.  
<<Index Size Comparison>>=
if(size() < rhs.size()) return -1;
if(size() > rhs.size()) return 1;
@

The element comparisons are a little trickier.  Equality is easily defined,
but greater than is another thing alltogether!  This comparison function will
be used for putting indices in order and checking for equality.  For this,
we'll use a lexicographical ordering where the comparison is the result
of comparing the first two non-equal elements.  If all elements match, 
then the indices are equal.
<<Index Element Comparison>>=
for(int i=0; i<size(); i++) 
{
   //check for greater than
   if(v[i] > rhs.v[i]) 
   {
      //that's it!  We're done!
      return 1;
   }
   
   //check for less than
   if(v[i] < rhs.v[i])
   {
      return -1;
   }
}
@

All that remains is to implement the standard set of comparison and
equality operators.  This is, of course, trivial when we use the compare
function.
\index[functions]{\tt Tensor::Index::operator<(rhs)}
\index[functions]{\tt Tensor::Index::operator<=(rhs)}
\index[functions]{\tt Tensor::Index::operator>(rhs)}
\index[functions]{\tt Tensor::Index::operator>=(rhs)}
\index[functions]{\tt Tensor::Index::operator==(rhs)}
\index[functions]{\tt Tensor::Index::operator!=(rhs)}
<<Index Comparison Operations>>=
virtual bool operator<(const Index &rhs)  const { return compare(rhs)  < 0; }
virtual bool operator<=(const Index &rhs) const { return compare(rhs) <= 0; }
virtual bool operator>(const Index &rhs)  const { return compare(rhs)  > 0; }
virtual bool operator>=(const Index &rhs) const { return compare(rhs) >= 0; }
virtual bool operator==(const Index &rhs) const { return compare(rhs) == 0; }
virtual bool operator!=(const Index &rhs) const { return compare(rhs) != 0; }
@


Another type of comparison we are interested in is bounds checking.
For this, we need to see if the index contains only 
elements that are all strictly less than some upper bound.  
\index[functions]{\tt Tensor::Index::within(bound)}
<<Index Comparison Operations>>=
virtual bool within(const Index &bound) const
{
   //bounds must match
   if(size() != bound.size()) return false;
   
   for(int i=0; i<size(); i++) 
   {
      if(v[i]>=bound.v[i]) return false;
   }
   
   //all is good!
   return true;
}
@

\section{Construction}
Having defined the storage mechanisms of the tensor class that we are 
creating, we now turn our attention to the construction and definition
of the tensors themselves.  When the tensors are first created, they
are filled with {\tt E() } values, and are defined by their dimension.
In this template, dimensions are immutable.


<<Tensor Member Variables>>=
Index ubound;   //The dimension upper bound
@


So then, after creation a basic {\tt Tensor} object will consist of a 
tensor of a fixed finite dimension which is filled with a default value.
The most basic sort of constructor which could accomplish this simply
populates thedimension
\index[functions]{\tt Tensor::Tensor(ubound)}
<<Tensor Constructors>>=
Tensor(const Index &ubound)
  : ubound(ubound) { }
@

Of course, it may be advantageous to allow construction from an
initilizer list for the dimension, but this is trivial given the above
constructor and as well as the {\tt Index} constructor.

\index[functions]{\tt Tensor::Tensor(dimensions)}
<<Tensor Constructors>>=
Tensor(std::initializer_list<int> dimensions)
  : Tensor(Index(dimensions)) { }
@

In either case, the result is a sparse tensor with no non-default value
components.  The tensor as constructed here has an immutable finite
dimension.


\section{Accessing Elements}
\subsection{Accessing by {\tt Index} Objects}
Now that we have a fully constructed tensor, we need a method to access the
elements.  We need the access to be able to both read and write individual
elements.  Of course, the most obvious way to do this would simply be to use
an {\tt Index} parameter into an index operator.  We will, of course, 
want to perform bounds checking on this.  If we are out of bounds, we'll 
throw an exception.

<<Includes>>=
#include <exception>
@

When accessing an element, it will come from one of two places. The
first is that it could be in the data map of the tensor.  If it is
not, then it must be the fill value.

\index{functions}{\tt Tensor::get(index)}
<<Tensor Operations>>=
virtual E get(const Index &index)
{
   <<Bounds Check>>

   //find the element in the data map
   if(data.find(index) != data.end()) {
      return data[index];
   } else {
      return E();
   }
}
@

When setting values, we place them into the datamap if they are not
the default value.  If, on the other hand, we are asked to assign
a default value, then we we will not place in the map.  If we are overwriting
a value in the map, in order to maintain sparsity, we must delete the
entry.

\index{functions}{\tt Tensor::set(index, value)}
<<Tensor Operations>>=
virtual void set(const Index &index, const E& value) 
{
   <<Bounds Check>>

   //handle storage of the default value
   if(value == E()) {
      if(data.find(index) != data.end()) {
         data.erase(index);
      }
   } else {
      data[index] = value;
   }
}
@

Of course, bounds checking is trivial given our ability to compare {\tt Index}
objects.  We will respond to an out of bounds error by throwing 
{\tt std::out\_of\_range}.

<<Bounds Check>>=
if(not index.within(ubound))
{
   throw std::out_of_range("Tensor index is out of range.");
}
@


Indexing by objects will work, and this approach makes a lot of sense
for a tensor object, but it is inconvenient.  A better way to go about
this would be to contruct a series of operators to allow tensors to
be accessed using stanadard array syntax.  
A na\"{\i}ve approach to this would be to build an operator with a signature
like this:

<<Na\"{\i}ve Accessor>>=
virtual E& operator[](const Index &i);
@

Unfortunately this approach will not work.  Remember that the tensors we 
are working with are assumed to be sparse, which in turn means that very
few of the elements will actually be resident in memory.  A default 
element does not have a position to be returned, and so a reference would
not be desirable.  Worse, assignment to a returned reference to the default
element would alter all default elements!  Moreover, how would the tensor
be able to insert a non-default item into its map if the index operator
returns a reference to an element?  In order to combat this, we will
need to use an object which can access the elements.  This will not 
be an iterator object, per se, because this object will be designed
to access only one element at a time.  For now, we can assume that we 
have an inner class {\tt Accessor} which can handle this for us, thus
our corrected index operator will look like this:

\index{functions}{\tt Tensor::operator[](index)}
<<Tensor Operations>>=
virtual Accessor operator[](const Index &index) 
{
   <<Bounds Check>>
   
   return Accessor(*this, index);
}
@


\subsection{Accessing by Integers}
Another desirable method of accessing the elements in the tensor is by 
multiple integer indexes.  That is to say, suppose we have a $3^{\mathrm{rd}}$
order tensor, {\tt t}, we could access the first element via something like
{\tt t[0][0][0]}.  In order to do this, we will need to build an {\tt Accessor}
with a partially complete {\tt Index}, and then further indexing on the
{\tt Accessor} will allow the completion of the indicies.  The left most
index is the one the {\tt Tensor} class will handle, and the rest will
be left to the {\tt Accessor} class.  This method of indexing will have
the basic shape of:

\index{function}{Tensor::operator[](i)}
<<Tensor Operations>>=
Accessor operator[](int i) 
{
   <<Build Partial Index>>
   
   return Accessor(*this, index);
}
@

Building the partial index is easy enough.  We will construct an empty
index and append the given index. 

<<Build Partial Index>>=
Index index;
index.append(i);
@

\subsection{The {\tt Accessor} Class}
<<Tensor Inner Class Prototypes>>=
class Accessor;
@

<<Tensor Inner Classes>>=
class Accessor 
{
public:
   <<Accessor Operations>>
private:
   <<Accessor Constructor>>
   <<Accessor Member Variables>>
   <<Accessor Friends>>
};
@

In order to complete the access methods, we must now turn our attention 
to the {\tt Accessor} class.  This class will mainly be used with anonymous
objects, though it could be potentially useful as stored variable.  However,
because it is tied to the {\tt Tensor} objects themselves, it should only
ever be constructed by the {\tt Tensor} class.  The goals operations of
this class are as follows:
\begin{itemize}
   \item An {\tt Accessor} object must be small as many will be created and
     destroyed during the operation of a {\tt Tensor}.
   \item The {\tt Accessor} objects must allow for assignment operations to
     be performed on type {\tt E}.
   \item The {\tt Accessor} objects must be castable to a type {\tt E}.  
     This allows basic arithmetic and such to be performed
\end{itemize}

To begin with, let's note what we need to store about the accessor.  Really,
all we need is a reference to a tensor and an index.  The index will 
be built as we go along, or it will be complete when the Accessor is 
constructed.

<<Accessor Member Variables>>=
Tensor &t;
Index index;
@

As mentioned before, the constructor will be a private constructor, and as 
implied in the previous subsection it will need to take a reference to a 
{\tt Tensor} and an index. Also, we should throw an exception if we
try to create an accessor with too many dimensions.

\index[functions]{\tt Tensor::Accessor::Accessor(t, index)}
<<Accessor Constructor>>=
Accessor(Tensor &t, const Index& index) : t(t), index(index) 
{
    if(index.size() > t.dim().size()) {
        throw std::out_of_range("Too many dimensions in Tensor Access");
    }
}
@

Of course, {\tt Tensor} needs access to the private constructor, and 
so it is a friend of the {\tt Accessor} class.
<<Accessor Friends>>=
friend Tensor;
@

We now turn our attention to the indexing of Accessor objects.  This will
occur when using the multiple integer index form of tensor access 
(as in {\tt t[i][j][k]}). Each index is added to the end of the
accessor index to form a new accessor 

\index[functions]{\tt Tensor::Accessor::operator[](i)}
<<Accessor Operations>>=
virtual Accessor operator[](int i)
{
   Accessor a(t, index);   //copy present accessor
   a.index.append(i);      //add to the end

   return a; //return the new accessor
}
@

<<Find Dimension Index>>=
if(di == index.size()) throw std::out_of_range("Too many dimensions in Tensor Access");
@


Now all that remains is to build the operations of the Acessor.  Chief among
these is the cast to type {\tt E}.  

\index[functions]{\tt Tensor::Accessor::operatorE()}
<<Accessor Operations>>=
virtual operator E()
{
   return t.get(index);
}
@



The astitute reader will notice that the cast does not return a reference
type.  The reason for this is that we must maintain careful control over 
assignment so as to maintain the sparse nature of the array.  So we will
create an assignment operator, which makes use of the {\tt set}
operation and returns the assigned value.

\index[functions]{\tt Tensor::Accessor::operator=(rhs)}
<<Accessor Operations>>=
virtual E operator=(const E &rhs)
{
   t.set(index, rhs);
   return rhs;
}
@

For convenience, we'll add some other arithemtic assignment operators.
Generally, we expect to get floating point types in the tensor, so 
operations like modulous and bitwise operators are not provided here.
Of course, given the above operators, these are trivial.
\index[functions]{\tt Tensor::Accessor::operator+=(rhs)}
\index[functions]{\tt Tensor::Accessor::operator-=(rhs)}
\index[functions]{\tt Tensor::Accessor::operator*=(rhs)}
\index[functions]{\tt Tensor::Accessor::operator/=(rhs)}
<<Accessor Operations>>=
virtual E operator+=(const E &rhs) { return (*this) = (E)(*this) + rhs; }
virtual E operator-=(const E &rhs) { return (*this) = (E)(*this) - rhs; }
virtual E operator*=(const E &rhs) { return (*this) = (E)(*this) * rhs; }
virtual E operator/=(const E &rhs) { return (*this) = (E)(*this) / rhs; }
@

And now we have a completed accessor object, and thus each element of the
array can be accessed and no default fill value will be stored in the
data map.


\subsection{Access Tensor Dimensions}
In addition to the tensor data, we also need to provide access to the 
tensor dimensions.  Clearly, this will fascilitate the construction of
general case functions and loops.  This is of course a trivial function.

\index[functions]{\tt Tensor::dim()}
<<Tensor Operations>>=
virtual Index dim() 
{
   return ubound;
}
@

\section{Iterators}
Another useful type of operation is iteration.  Of course, there are 
multiple ways we could go about this.  For example, we could simply write
loops which iterate over the tensor using the {\tt Accessor} objects.
This will work well, though the coding could be a little tricky if we 
don't know the order of the tensor at coding time.  Being able to iterate
over each element is, of course, a fundamental operation of any container,
and our multidimensional tensor is no exception!

Another form iteration that we are interested in is iterating over the
non-sparse elements.  This is a crucial step for some tensor
operations, and it is also needed for storing tensors in files.

\subsection{Full (Dense) Iterator} 
The first type of iterator to consider is the full, or dense,
iterator.  This iterator will visit every possible index in the
tensor, even if it is a sparse element.

First, let's think about the anatomy of an iterator for a tensor.  We
will, of course, need an inner class to handle this.

<<Tensor Inner Class Prototypes>>=
class Iterator;
@

And we will need a fairly standard layout for the iterator itself.

<<Tensor Inner Classes>>=
class Iterator
{
public:
    <<Tensor Iterator Constructors>>
    <<Tensor Iterator Movement>>
    <<Tensor Iterator Comparison>>
    <<Tensor Iterator Accessors>>
protected:
    <<Tensor Iterator Private Members>>
};
@

Because everything in the {\tt Tensor} class is based on indices, we need
to maintain current position as an index.
<<Tensor Iterator Private Members>>=
Index cur;  //current position
@

Also, we need to tie the {\tt Iterator} object back to the {\tt Tensor} 
object which spawned it, and so we need to allow for that.
<<Tensor Iterator Private Members>>=
Tensor *t;  //the tensor we are iterating over
@

Finally, we need to keep track of the upper bound of {\tt Tensor} indices.
We could do this by repeated calls to {\tt t->dim()}, but this would be
ineffecient.  The dimensions of our {\tt Tensor} objects are immutable, 
and so we can safely store a ubound for convenience.
<<Tensor Iterator Private Members>>=
Index ubound; //the upper bound of the tensor we are iterating over
@

\subsubsection{Tensor Iterator Construction}
Construction of the iterator is a fairly straightforward task.  First, we
will have a no argument constructor which will do nothing.

\index[functions]{\tt Tensor::Iterator::Iterator()}
<<Tensor Iterator Constructors>>=
Iterator() 
{
   //do nothing
}
@

Next, we should provide a copy constructor, which is trivial.

\index[functions]{\tt Tensor::Iterator::Iterator(rhs)}
<<Tensor Iterator Constructors>>=
Iterator(const Iterator& rhs)
{
   cur = rhs.cur;
   t = rhs.t;
   ubound = rhs.ubound;
}
@

And finally, we create a constructor which receives an index and a tensor
pointer and initializes everything accordingly.
<<Tensor Iterator Constructors>>=
Iterator(const Index &cur, Tensor *t) : cur(cur), t(t), ubound(t->dim()) {} 
@

That last constructor is the one that will be most frequently used,
and in fact we need to add begin and end operations to {\tt Tensor}.

Iteration begins at an index of all 0's.

\index[functions]{\tt Tensor::begin()}
<<Tensor Operations>>=
virtual Iterator begin()
{
   return Iterator(Index(ubound.size(), 0), this);
}
@

The iterator that is just past the end of the {\tt Tensor} is the one who's
index is equal to the ubound of the tensor.

\index[functions]{\tt Tensor::end()}
<<Tensor Operations>>=
virtual Iterator end()
{
   return Iterator(ubound, this);
}
@


\subsubsection{Tensor Iterator Movement}
The {\tt Iterator} object is a biderectional iterator, so we will provide
both increment and decrement operations.  In addition, we also provide
arithmetic operations.  All of these will be faciliated by a private
method which will handle the actual operations.

First, we create the increment operations.

\index{function}{Tensor::Iterator::operator++()}
\index{function}{Tensor::Iterator::operator++(int)}
<<Tensor Iterator Movement>>=
virtual Iterator& operator++()
{
   move(1);
   return *this;
}

virtual Iterator operator++(int) 
{
   Iterator result(*this);
   move(1);
   return result;
}
@

\index{function}{Tensor::Iterator::operator--()}
\index{function}{Tensor::Iterator::operator--(int)}
Now we create the decrement operations
<<Tensor Iterator Movement>>=
virtual Iterator& operator--()
{
   move(-1);
   return *this;
}


<<Tensor Iterator Movement>>=
virtual Iterator operator--(int)
{
   Iterator result(*this);
   move(-1);
   return result;
}
@

And finally, the arithmetic operators for adding and subtracting by 
integers.

\index[functions]{\tt Tensor::Iterator::operator+(int rhs)}
\index[functions]{\tt Tensor::Iterator::operator-(int rhs)}
<<Tensor Iterator Movement>>=
virtual Iterator operator+(int rhs)
{
   Iterator result(*this);
   result.move(rhs);
   return result;
}


virtual Iterator operator-(int rhs)
{
   Iterator result(*this);
   result.move(-rhs);
   return result;
}
@

This, of course, leaves the detail of the movement function itself.
This will be a private method which will advance the iterator by one 
position in either the positive or negative direction.  It will visit 
each item in the tensor in a left-most index major way, advance the 
other indices as the left ones over or under flow.

\index[functions]{\tt Tensor::Iterator::move(d)}
<<Tensor Iterator Private Members>>=
virtual void move(int d) 
{
    int i;          //a general index variable

    if(d > 0) 
    {
       <<Tensor Iterator Forward Movement>>
    } 
    else if(d < 0) 
    {
       <<Tensor Iterator Backward Movement>>
    }
}
@

If we imagine tensors as multi-dimensional arrays, then it follows
that we adopt an indexing convention $i_1, i_2, \ldots, i_k$ where
$i_1$ is analagous to a ``row'', and $i_2$ is analagous to
a ``column''.  Moreover $i_3$ is a layer, thus giving us a series of
stackedmatrices.  Indexes $i_4, \ldots, i_k$ would be higher
dimensional constracts without any readily apparent intuitive meaning.  

Given this arrangement of indexes, a natural way for an iterator to
move through the tensor is in row-major order.  That is, it iterates
through all columns within a row before moving on to the next.
Following the exhaustion of all rows, it increments the layer index,
after which it will move on to the higher dimensional layers each in
turn.  The only complication lies in the initial row-major traversal.
That is, we must increment the second index before the first (If such
an index exists.)  

The task for iterator increment is thus summarized as:

<<Tensor Iterator Forward Movement>>=
<<Select Starting Index>>
while(i < cur.size()) {
    //increment the index, if is valid we are done!
    cur[i] += d;
    if(cur[i] < ubound[i]) {
        return;  
    }

    //Fix the overflow!
    d=cur[i]/ubound[i];
    cur[i] %= ubound[i];

    <<Select Next Index>>
}
@

Selecting the starting index is really just contingent on whether
there is more than one index to increment.
<<Select Starting Index>>=
if(cur.size() > 1) {
    i = 1;
} else {
    i = 0;
}
@

Updating the next index is relatively simple.  There are only two
special cases (when i=1 and i=0).
<<Select Next Index>>=
if(i == 1) {
    i = 0;  //update the row next
} else if(i == 0) {
    i = 2;  //update the layer next
} else {
    i++;    //go to the next high-dimensional layer
}
@

If the forward motion makes it to the end, then that means that we have
exhausted all the positions for the index.  We treat this as a special 
condition in that we have just moved past the end.  In this situation,
we simply set the index to the upper bound so that comparison with the
result of {\tt end()} will work properly.

<<Tensor Iterator Forward Movement>>=
cur = ubound;
@

Backward motion begins with a special case, namely the case where we are
at the {\tt end()} position.  In this situation, we go to the last
valid index, which takes care of going back by 1 index.

<<Tensor Iterator Backward Movement>>=
if(cur == ubound) 
{
  for(int i=0; i<cur.size(); i++) 
  {
    cur[i]--;
  }
  d--;
}
@

Now, we finish the backward motion by the same type of addition and 
correction pattern.  We again move in a row-major fashion, the only
real difference is in the underflow mechanism.

<<Tensor Iterator Backward Movement>>=
<<Select Starting Index>>
while(i < cur.size()) {
    //increment and validate
    cur[i]+=d;
    if(cur[i] >= 0) 
    {
        return;  //all done!
    } 
   
   //find out how much to "borrow" form the next column and adjust this
   //column accordingly
   d = cur[i] / ubound[i] - (cur[i] % ubound[i] ? 1 : 0);
   cur[i] += ubound[i] * -d;

   <<Select Next Index>>
}
@

We will not do anything about stepping beyond an all 0 index, and will 
instead leave that to the programmer who uses this class.

\index[functions]{\tt Tensor::Iterator::operator<(rhs)}
\index[functions]{\tt Tensor::Iterator::operator<=(rhs)}
\index[functions]{\tt Tensor::Iterator::operator>(rhs)}
\index[functions]{\tt Tensor::Iterator::operator>=(rhs)}
\index[functions]{\tt Tensor::Iterator::operator==(rhs)}
\index[functions]{\tt Tensor::Iterator::operator!=(rhs)}


\subsubsection{Tensor Iterator Comparison}
Comparison of the iterators is really just comparison of the indexes.
<<Tensor Iterator Comparison>>=
virtual bool operator<(const Iterator &rhs)  { return cur <  rhs.cur; }
virtual bool operator<=(const Iterator &rhs) { return cur <= rhs.cur; }
virtual bool operator>(const Iterator &rhs)  { return cur >  rhs.cur; }
virtual bool operator>=(const Iterator &rhs) { return cur >= rhs.cur; }
virtual bool operator==(const Iterator &rhs) { return cur == rhs.cur; }
virtual bool operator!=(const Iterator &rhs) { return cur != rhs.cur; }
@

\subsubsection{Tensor Iterator Accessors}
Element access, as provided by dereferencing an {\tt Iterator} object 
provides access to a Tensor Accessor, just as we would when indexing.
Thus we can avoid repeating a great deal of logic.
\index[functions]{\tt Tensor::Iterator::operator*()}
<<Tensor Iterator Accessors>>=
virtual Accessor operator*()
{
   return (*t)[cur];
}
@

Another point of interest is the index itself.  For this, we add a 
function which returns the index.
\index[functions]{\tt Tensor::Iterator::index()}
<<Tensor Iterator Accessors>>=
virtual Index index()
{
   return cur;
}
@


\subsection{Sparse Iterator}
Sparse iteration is useful in situations where we only care about the
non-sparse elements of the tensor.  This is the case when we are
trying to store it in a file, or in some algorithms associated with
tensor arithmetic.  Fortunately, the beavior of a sparse iterator is
close enough to that of a dense iterator that we can simply inherit
it.  The chief difference will be in construction and movement.

<<Tensor Inner Class Prototypes>>=
class SparseIterator;
@

<<Tensor Inner Classes>>=
class SparseIterator : public Iterator
{
public:
    <<Sparse Iterator Constructors>>

protected:
    <<Sparse Iterator Movement>>
    <<Sparse Iterator Private Variables>>
};
@

Clearly, this iterator will only need concern itself with the elements
actually stored in the data map within the tensor object.  We may be
tempted to store an iterator into this map and then move from there,
however this could lead to a problem which will be further explained
in a later section.  Instead, what we want to do is store a pointer to
the map we will be exploring.

<<Sparse Iterator Private Variables>>=
std::map<Index, E> *dptr;
@

The rest of the fields will simply come from the base class.

\subsubsection{Sparse Iterator Construction}
Constructing a sparse iterator is similar to the construction of
a dense iterator.  First, for convenience, we'll include a no argument
constructor which does nothing.

<<Sparse Iterator Constructors>>=
SparseIterator()
{
    //do nothing
}
@

We should also create a copy constructor, which again is simply
copying the fields of the iterator and using the base copy constructor
for the base stuff.
<<Sparse Iterator Constructors>>=
SparseIterator(const SparseIterator &rhs) : Iterator(rhs)
{
    dptr = rhs.dptr;
}
@


The final constructor to consider is the one actually associates the
iterator with its container.  This has an analagous relationship to
the same constructor in the dense iterator, and we can use the dense
constructor.  The only extra step is the storage of the datamap
pointer, and so this can all be accomplished using the initializer
list of the constructor.

<<Sparse Iterator Constructors>>=
SparseIterator(const Index &cur, Tensor *t, std::map<Index, E> *dptr)
  : Iterator(cur, t), dptr(dptr) { }
@


The final construction consideration is in the production of the begin
and end point of the sparse iterator.  This is an addition to the
tensor operations.  The first index in the map is the first non-sparse index.

<<Tensor Operations>>=
virtual SparseIterator sparseBegin()
{
    Index idx;   //the beginning index

    //Find the first non-sparse index (if there is one)
    if(data.empty()) {
       idx = ubound;
    } else {
        idx = data.begin()->first;
    }

    return SparseIterator(idx, this, &data);
}
@

Of course, we also need to find the end of the tensor.  This could be
more complicated, but as we shall see in the movement functions, all
we need to do is use the ubound.

<<Tensor Operations>>=
virtual SparseIterator sparseEnd()
{
    return SparseIterator(ubound, this, &data);
}
@

\subsection{Sparse Iterator Movement}
All that is let is to create the sparse iterator's movement.  Recall
that we have opted to not create an internal iterator with reference
to the data map.  The reason for this is that it is entirely possible
that a user will assign the 0 value to the iterator.  This would, in
turn, result in the current index being removed from the data map,
which would invalidate our internal iterator.  Instead, what we need
is to base everything off of the index. The indexes are always value,
though sometimes they will refer to a sparse element, so the real
trick is finding indexes within the map at all times.

<<Sparse Iterator Movement>>=
virtual void move(int d)
{
    typename std::map<Index, E>::iterator itr;   //map iterator

    <<Find Map Iterator to Current Index>>
    <<Move Sparse Index>>
    <<Cleanup Map>>
}
@

To find the current map iterator, we just have to find the current
index within the map.  However, because this may be an index that is
absent (assigned the E() value), we need to visit it once to ensure
that it is present in sorted order within the map. (for iteration
purposes.)

<<Find Map Iterator to Current Index>>=
(*dptr)[Iterator::cur];  //get the element in the map (perhaps temporarily)
itr = dptr->find(Iterator::cur);  //get an iterator to that index
@

Now we just move the iterator and extract the current index.

<<Move Sparse Index>>=
if(d < 0) {
    //move backwards
    itr--;
} else  {
    //move forwards
    itr++;
}
@

Extracting the index has two possibilities.  One is that we've reached
the end of the map, in which case we set it to the upper boundary
(dimension) of the tensor.  Otherwise, it just comes from the iterator
<<Move Sparse Index>>=
if(itr == dptr->end()) {
    Iterator::cur = Iterator::t->dim();
} else {
    Iterator::cur = itr->first;
}
@


Finally, there exists a potential for our method of getting the index
to put zero elements into the data map.  We need to fix this.
<<Cleanup Map>>=
if(itr->second == E()) {  //remove zero elements
    dptr->erase(itr->first);
}
@

\section {Tensor Arithmetic}
Having constructed, accessed, and stored tensor data, we now turn our 
attention to the arithmetic operations of the tensor.  These fall into two
groups, scalor and tensor operations.  Scalar operations are interactions
between a tensor and a single value, while tensor operations either 
involve only the tensor itself or the interaction between two tensors.

\subsection{Scalar Operations}
Scalars can be used to multiply and divide tensors.  These operations
are simply applied to all elements within the tensor.  The first operation
is multiplication.  This operation is most easily expressed in terms of 
the assignment multiplication operator:

\index[functions]{\tt Tensor::operator*=(s)}
<<Tensor Scalar Operations>>=
virtual Tensor& operator*=(const E& s)
{
   //multiply all the "non-empty" cells
   for(auto itr = sparseBegin(); itr != sparseEnd(); itr++) {
      *itr *= s;
   }
   
   return *this;
}
@

Of course, it is also desirable to implement the * operator, so that we 
can return a scaled copy of a Tensor.  This is trivial given the above
operator.

\index[functions]{\tt Tensor::operator*(s)}
<<Tensor Scalar Operations>>=
virtual Tensor operator*(const E& s)
{
   Tensor result(*this);
   
   return result*=s;
}
@

Likewise, scalar division is simply dividing all the elements by the scalar
value.

\index[functions]{\tt Tensor::operator/=(s)}
\index[functions]{\tt Tensor::operator/(s)}
<<Tensor Scalar Operations>>=
virtual Tensor& operator/=(const E& s)
{
   //multiply all the "non-empty" cells
   for(auto itr = sparseBegin(); itr != sparseEnd(); itr++) {
      *itr= s;
   }
   
   return *this;
}
@


<<Tensor Scalar Operations>>=
virtual Tensor operator/(const E& s)
{
   Tensor result(*this);
   
   return result/=s;
}
@

\subsection{Tensor Arithmetic Operations}
In addition to scalar operations, a tensor can also undergo several
arithmetic operations with other tensors.  

\subsubsection{Tensor Addition and Subtraction}
The fist, and perhaps easiest,
to implement is Tensor addition.  Tensor addition is performed element by 
element on two tensors of like dimension.  Given two Tensors $C=A+B$ is 
defined where each element in $C$ is given as:
\[
C_{i_1 \ldots i_k} = A_{i_1 \ldots i_k} + B_{i_1 \ldots i_k}
\]

A straightforward approach to this would be to simply iterate through 
all the indexes and perform the addition.  This would, of course, be 
an operation with a time complexity in $\Theta(n^k)$.  While polynomial,
this is not the best we can do!  Given that we are storing sparse 
Tensors, we can do better by only adding the non-fill values together.

We will start with the addition assignment operator.

\index[functions]{\tt Tensor::operator+=(rhs)}
<<Tensor Operations>>=
virtual Tensor& operator+=(Tensor &rhs) 
{
   //the new fill value is the sum of the two fill values
   <<Add non-fill positions from the left>>
   <<Add non-fill positions from the right>>
   
   return *this;
}
@

Starting with the left hand side, we will pull the non-fill positions
and add the corresponding elements from the right hand side.

<<Add non-fill positions from the left>>=
for(auto itr = sparseBegin(); itr != sparseEnd(); itr++) 
{
   *itr += rhs[itr.index()];
}
@

Now we need to capture the non-zero elements on the right that have not
been added yet.  These will be the ones without a corresponding
entry in the left's {\tt data} map.

<<Add non-fill positions from the right>>=
for(auto itr = rhs.sparseBegin(); itr != sparseEnd(); itr++) 
{
   if(data.find(itr.index()) == data.end()) {
      set(itr.index(), *itr);
   }
}
@


As with other addition operators, we can now implement the addition of
two {\tt Tensor} objects using the addition assignment operator.

\index[functions]{\tt Tensor::operator+(rhs)}
<<Tensor Operations>>=
virtual Tensor operator+(Tensor &rhs) 
{
   Tensor result(*this);
   return result+=rhs;
}
@

Subtraction is implemented the same way.
\index[functions]{\tt Tensor::operator-=(rhs)}
\index[functions]{\tt Tensor::operator-(rhs)}

<<Tensor Operations>>=
virtual Tensor& operator-=(Tensor &rhs) 
{
   <<Subtract non-fill positions from the left>>
   <<Subtract non-fill positions from the right>>
   return *this;
}
@

<<Subtract non-fill positions from the left>>=
for(auto itr = sparseBegin(); itr != sparseEnd(); itr++) 
{
   *itr -= rhs[itr.index()];
}
@

<<Subtract non-fill positions from the right>>=
for(auto itr = rhs.sparseBegin(); itr != rhs.sparseEnd(); itr++) 
{
   if(data.find(itr.index()) == data.end()) {
      set(itr.index(), E() - *itr);
   }
}
@

<<Tensor Operations>>=
virtual Tensor operator-(Tensor &rhs) 
{
   Tensor result(*this);
   return result-=rhs;
}
@

\subsection{Tensor Products}
Several product operations are possible for tensors. The products
implemented for this tensor are:
\begin{itemize}
    \item Outer or Tensor Product
    \item Hadamard Product
    \item Kronecker Product
    \item Khatri-Rao Product
\end{itemize}

\subsubsection{Outer Product}
The outer product, also known as the tensor product, is a product
between two tensors and yields another tensor.  Given two tensors $A$
and $B$, the outer product can be written as $C = A \circ B$. 

This productis implemented by the {\tt outer} function, which
multiplies the current object (which we will call $A$) with another
tensor object (which we will refer to as $B$).

<<Tensor Operations>>=
virtual Tensor outer(Tensor &B)
{
    <<Outer Product Dimension>>
    <<Outer Product Multiplication>>

    return C;
}
@

The new tensor, $C$, must be generated with the proper dimension.  The
result of the outer product will have the dimensions 
\[
C.\mathrm{dim} \leftarrow A.\mathrm{dim} \oplus B.\mathrm{dim}
\]

<<Outer Product Dimension>>=
Tensor C(Index{dim(), B.dim()});
@

To complete the product, we populate the tensor $C$ with the actual
multiplications.  Suppose $A$ is a tensor of order $k$ and $B$ is
a tensor of order $l$. Clearly, $C$ is a tensor of order $k+l$. Each
element of $C$ is computed as
\[
C_{i_1\ldots i_k,j_1\ldots j_l} = A_{i_1\ldots i_k}
B_{j_1\ldots j_l}
\]

This operation can be made more effecient by taking advantage of the
sparse expectation of the tensor object.  Any element populated by
a zero need not be multiplied, and so we can arrive at this product by
simply iterating over the non-zero elements in both $A$ and $B$.  All
other elements in the tensor will, of course, be 0.


<<Outer Product Multiplication>>=
for(auto aitr = sparseBegin(); aitr!=sparseEnd(); aitr++) {
    for(auto bitr = B.sparseBegin(); bitr!=B.sparseEnd(); bitr++) {
        C.set(Index{aitr.index(), bitr.index()}, *aitr * *bitr);
    }
}
@

\printindex[functions]
\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
